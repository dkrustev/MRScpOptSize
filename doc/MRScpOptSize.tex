\documentclass[submission,copyright,creativecommons]{eptcs}
\providecommand{\event}{VPT 2020} % Name of the event you are submitting to
\usepackage{breakurl}             % Not needed if you use pdflatex only.
\usepackage{underscore}           % Only needed if you use pdflatex.

\usepackage{listings}
\lstset{
  language=Caml,
  defaultdialect=[Objective]Caml,
  basicstyle=\footnotesize,
  frame=tb
  % basicstyle=\small\ttfamily,
  %	commentstyle=\textit,
  %	keywordstyle=\bfseries
}

\usepackage{stmaryrd} % for \{ll,rr}bracket	
\usepackage{caption,subcaption}

\title{Optimizing Program Size Using Multi-result Supercompilation}
\author{Dimitur Nikolaev Krustev
\institute{IGE+XAO Balkan\\ Sofia, Bulgaria}
\email{\quad dkrustev@ige-xao.com}
}
\def\titlerunning{Optimizing Program Size Using MRSC}
\def\authorrunning{D.N. Krustev}
\begin{document}
\maketitle

\begin{abstract}
This is a sentence in the abstract.
This is another sentence in the abstract.
This is yet another sentence in the abstract.
This is the final sentence in the abstract.
\end{abstract}

\section{Introduction}

Supercompilation was invented by Turchin \cite{TurchinSupercompilerConcept} and has found numerous
applications, such as program optimization\cite{Sorensen1994TurchinSupercompiler,sorm98b,TMR/SCP2014}, 
program verification \cite{Klyuchnikov2010,MendelGleasonPhD2011},
$\ldots$.

Supercompilation performs very powerful program transformation by simulating
the actual execution of the input program on a whole set of possible inputs
simultaneously.
The flip side of this power is that the behavior of supercompilation -- 
with respect to both transformation time and result size --
can be very unpredictable.
This fact makes supercompilation problematic for including as an
optimization step of a standard compiler, for example.
Measures have been proposed to make supercompilation more
well-behaved, both in execution time and result size \cite{bolingbroke2011improving,Jonsson2011Taming}.
These proposals are all based on a combination of specially crafted and
empirically fine-tuned heuristics.
The main goal of the present study is to experiment with a more
principled approach to optimizing the size of the program resulting
from supercompilation.
This approach is based on a couple of key ideas:
\begin{itemize}
  \item use multi-result supercompilation\footnote{often abbreviated as \emph{MRSC} from now on} 
    \cite{KlyuchnikovMRSCBranch,Romanenko2014StagedMRSC}
    to systematically explore a large set of different generalizations during 
    the transformation process, 
    leading to different trade-offs between performed optimizations and code explosion;
  \item carefully select a generalization scheme, which can avoid all forms of
    code duplication if applied systematically;
  \item re-use ideas from Grechanik et al. \cite{Romanenko2014StagedMRSC} to compactly represent and efficiently
    explore the set of programs resulting from multi-result supercompilation.
\end{itemize}
We outline the main ideas of multi-result supercompilation --
as well as the specific approach to its implementation that we use --
in Sec. \ref{sec:MRSCSummary}. We then describe the main contributions of this study:
\begin{itemize}
  \item We define a particular strategy for generalization during MRSC (Sec. \ref*{sec:Generalize}), which:
    \begin{itemize}
      \item avoids any risk of code duplication (when applied);
      \item avoids unnecessary increase of the search space of possible
        transformed programs, which MRSC must explore
    \end{itemize}
  \item We analyze the performance of the proposed strategy on several
    simple examples (Sec. \ref*{sec:EmpEval}).
\end{itemize}

\section{Summary of Multi-result Supercompilation}\label{sec:MRSCSummary}

Most supercompilers perform generalization of the current configuration only
when forced to do so by the whistle.
The only subtlety, as suggested by S{\o}rensen et al. \cite{sorm98b} is to select
whether to generalize the lower or the upper configuration of the two that
have caused the whistle.
One of the key insights behind multi-result supercompilation is that the place
where the whistle has blown is not always the best place to make a generalization.
The proposed solution is radical: do not generalize when the whistle has blown,
but at any driving step, where a suitable generalization exists;
pursue in parallel all possible combinations of driving/generalizing 
the current configuration and its descendants.
This parallel analysis of alternative configurations is the source
of the multiple resulting programs.

Our implementation of multi-result supercompilation mostly follows the same
generic framework used in \cite{Romanenko2014StagedMRSC,krustev2014approach}.
It offers some important simplifications compared by the original work
of Klyuchnikov et al. \cite{KlyuchnikovMRSCBranch,Klyuchnikov:META2012:MRSC}:
\begin{itemize}
  \item It is based on big-step driving.
  \item All the set of transformed programs is represented compactly
    in a tree-like data structure, which further permits not only
    recovering the full set of configuration graphs, but also
    performing efficiently some kinds of queries on this set.
\end{itemize}
The compact representation of the set of graphs is shown in Fig. \ref{fig:GraphSet}.
We use direct excerpts of the F\# code of the implementation, but hopefully
they will be readable by anyone familiar with other functional languages such as OCaml and Haskell.
Another important caveat is that we have implemented MRSC for programs in a specific language,
so some details will only become clear once we introduce this language.
Still, the details of the language are not important for understanding the core
MRSC algorithm; indeed, such details are successfully abstracted away in \cite{Romanenko2014StagedMRSC,krustev2014approach}.
We prefer to show excerpts from our actual implementation for concreteness.
This representation is termed \emph{lazy graph} by Grechanik et al. \cite{Romanenko2014StagedMRSC},
and can be viewed as a domain-specific language (DSL) describing the construction of the
complete set of configuration graphs produced by multi-result supercompilation.
\begin{itemize}
  \item Node \verb|GSNone| is used when the whistle has blown. 
    It represents an empty set of configuration graphs;
  \item Node \verb|GSFold| is used when folding is possible. 
    It gives the relative distance to the upper node to which we fold, 
    plus the renaming, which makes the folded configurations compatible.
  \item Node \verb|GSBuild| is the most complicated one, representing a list
    of alternative developments (driving or generalization) of the current configuration.
    Each alternative, in turn, gives rise to a list of new configurations to explore,
    and hence, to a list of nested graph sets.
\end{itemize}

\begin{figure}
\begin{lstlisting}
type GraphSet =
  | GSNone
  | GSFold of MConf * int * list<VarName * VarName>
  | GSBuild of MConf * list<list<GraphSet>>

let rec gset2graphs (gs: GraphSet) : seq<MConf * ConfGraph> =
  match gs with
  | GSNone -> Seq.empty
  | GSFold(conf, n, ren) -> seq{ yield (conf, CGFold(n, ren)) }
  | GSBuild(conf, xss) ->
    xss
    |> Seq.collect (fun xs -> xs |> Seq.map gset2graphs |> Seq.cartesian)
    |> Seq.map (fun cgs -> (conf, buildGraph (snd conf) cgs))
\end{lstlisting}
\caption{Representation and Expansion of Graph Sets}
\label{fig:GraphSet}
\end{figure}

The semantics of this DSL is shown in the same Fig. \ref{fig:GraphSet} as
a function \verb|gset2graphs| expanding a \verb|GraphSet| into a sequence 
of configuration graphs.
Note the use of \verb|Seq.cartesian| to compose the sub-graphs of 
the graph node of each alternative configuration.

\begin{figure}
\begin{lstlisting}
type private HistEntryKind = HEGlobal | HELocal

let rec private mrScpRec (defs: Defs) (nestLvl: int) (hist: 
  list<HistEntryKind * int * Exp>) (conf: MConf) : GraphSet =
  let (_, e) = conf
  let relevantHist hek hist =
    match hek with
    | HELocal -> hist |> List.takeWhile (fun (hek, _, _) -> hek = HELocal)
    | HEGlobal -> hist |> List.filter (fun (hek, _, _) -> hek = HEGlobal)
  match List.tryFindSome (fun (_, _, e1) -> renaming e1 e) hist with
    | Some ((_, lvl, _), ren) -> GSFold(conf, nestLvl - lvl, ren)
    | None ->
      let rs = multiDriveSteps defs e
      let hek = if List.tryFind (fun mdsr -> 
                  match mdsr with MDSRCases _ -> true | _ -> false) rs = None
                then HELocal else HEGlobal
      let relHist = relevantHist hek hist
      match List.tryFind (fun (_, _, e1) -> homeomorphicEmbedding e1 e) relHist with
      | Some _ -> GSNone
      | None ->
        let confss = rs |> List.map (fun mdsr -> 
          mdsrSubExps mdsr |> List.map (fun e -> (mdsr, e)))
        let newHist = (hek, nestLvl, e)::hist
        GSBuild(conf, List.map (List.map (mrScpRec defs (nestLvl + 1) newHist)) confss)

let mrScp (defs: Defs) (e: Exp) : GraphSet =
  mrScpRec defs 0 [] (MDSRUnfold e, e)
\end{lstlisting}
\caption{Main MRSC Algorithm}
\label{fig:MRSCAlg}
\end{figure}

The main MRSC algorithm -- the one that builds the graph set of a given initial
configuration -- is shown in Fig. \ref{fig:MRSCAlg}.
We can ignore the details about splitting the configuration history into local
and global one -- they mostly follow established heuristics as in S{\o}rensen et al. 
\cite{Sorensen1994TurchinSupercompiler,sorm98b}.
The overall approach is simple - if folding is possible, we produce a fold node 
and stop pursuing the current configuration.
Otherwise we check the whistle -- in our case, the now standard homeomorphic embedding relation.
If it blows, we stop immediately with an empty set of resulting graphs.
When there is neither folding nor a whistle, we continue analyzing the execution of the
current configuration -- based on 2 language-specific functions:
\begin{itemize}
  \item \verb|multiDriveSteps| returns a number of alternatives for the current configuration --
    either a driving step, or (possibly several different forms of) generalization.
  \item \verb|mdsrSubExps| returns -- for a given alternative produced by the previous function --
    the list of sub-configurations (in our case -- sub-expressions) that must be subjected
    to further analyzis.
\end{itemize}
The implementation of both functions is described in Sec. \ref{sec:Generalize}.
Once we have this list of lists of sub-configurations, we simply apply the same
algorithm recursively, but with extended history.
Readers familiar with the implementation details of other supercompilers are
invited to compare them to the simplicity of this MRSC approach.

\section{Generalization Approach}\label{sec:Generalize}

\subsection{Programming Language}

The object language we consider is a first-order functional language with ordinary and 
pattern-matching function definitions.
Its syntax is summarized in Fig. \ref{fig:SLLsyntax}.
A very similar language is often used in many introductions to positive
supecompilation \cite{Sorensen1994TurchinSupercompiler,sorm98b,TMR/SCP2014}.
A notable restriction in our case is that we omit if-expressions and a built-in generic equality.
We assume the language has a call-by-name semantics, as this case is easiest to supercompile,
but we skip a more detailed presentation of the semantics of the language, assuming it mostly obvious.

\begin{figure}
\begin{tabular}[t]{l r l@{\hspace{20pt}} l}
$e$ & ::= && expression   \\
    && $x$ & variable     \\
    && $a(e_1,\ldots,e_n)$ & call \\
$a$ & ::= && call kind    \\
    && $C$ & constructor  \\
    && $f$ & function     \\
$p$ & ::= & $C(x_1, $\ldots$, x_n)$ & pattern \\
$d$ & ::= && function definition \\
    &         & $f(x_1, \ldots, x_n) = e$ & ordinary function \\
    & $\mid$  & $g(p_1, y_1, \ldots, y_m) = e_1$ & pattern-matching function \\
    &         & $\ldots$ & \\
    &         & $g(p_n, y_1, \ldots, y_m) = e_n$ & \\
$P$ & ::= & $d_1, \ldots, d_n$ & program
\end{tabular}
\caption{Object Language Syntax}
\label{fig:SLLsyntax}
\end{figure}

\subsection{Driving}

Let us recall how driving looks like for this simple language, in the case of standard (positive) supercompilation.
As a technical device, we define a single step of driving, producing a
result of type \verb|DriveStepResult|, as defined in Fig. \ref{fig:DriveStepResult}

\begin{figure}
\begin{lstlisting}
type DriveStepResult =
  | DSRNone
  | DSRCon of ConName * list<Exp>
  | DSRUnfold of Exp
  | DSRCases of VarName * list<Pattern * Exp>
\end{lstlisting}
\caption{Result of a Single Step of Driving}
\label{fig:DriveStepResult}
\end{figure}

\begin{itemize}
  \item We cannot drive a variable any further: $\mathtt{drive} \llbracket x \rrbracket = \mathtt{DSNone}$;
  \item Driving a constructor results in a constructor node with all arguments available for
    further driving: $\mathtt{drive} \llbracket C(e_1, \ldots, e_n) \rrbracket = \mathtt{DSCon}(C, e_1, \ldots, e_n)$;
  \item If we stumble upon a call to an ordinary function, we simply unfold its definition:
    $\mathtt{drive} \llbracket f(e_1, \ldots, e_n) \rrbracket = \mathtt{DSUnfold}(e [ x_1\rightarrow e_1, \ldots, x_n\rightarrow e_n ])$,
    where $f(x_1, \ldots, x_n) = e \in P$ and $ [ x_1\rightarrow e_1, \ldots, x_n\rightarrow e_n ] $
    denotes simultaneous substitution;
  \item The most intersting cases concern a call to a pattern-matching function, as the situation is
    differnt depending on the kind of the first argument:
    \begin{itemize}
      \item $\mathtt{drive} \llbracket g(C(e'_1, \ldots, e'_m), e_1, \ldots, e_n) \rrbracket =$
        $\mathtt{DSUnfold}(e [ x_1\rightarrow e'_1, \ldots, x_m\rightarrow e'_m, y_1 \rightarrow e_1, \ldots, y_n \rightarrow e_n ])$
        where $g(C(x_1, \ldots, x_m), y_1, \ldots, y_n) \in P$;
      \item $\mathtt{drive} \llbracket g(x, e_1, \ldots, e_n) \rrbracket =$
        $\mathtt{DSCases}(x, \mathtt{propagate}(x, p_1, (e_1, \ldots, e_n), e'_1),$ $\ldots,$ \\
          $\mathtt{propagate}(x, p_m, (e_1, \ldots, e_n), e'_m))$
        where $g(p_1, y_1, \ldots, y_n) = e'_1, \ldots, g(p_m, y_1, \ldots, y_n) = e'_m \in P$
        and $\mathtt{propagate}$ performs positive information propagation by substituting (a suitable renaming of) $p_i$ for $x$ 
        in the corresponding branch $i$;
      \item $\mathtt{drive} \llbracket g(f(e'_1, \ldots, e'_m), e_1, \ldots, e_n) \rrbracket =$
        $\mathtt{dsrMap}(\llbracket g(\bullet, e_1, \ldots, e_n) \rrbracket, \mathtt{drive} \llbracket f(e'_1, \ldots, e'_m) \rrbracket)$
        where $\mathtt{dsrMap}$ transforms a driving step result by splicing it in an expression with a hole.
    \end{itemize}
\end{itemize}
We deliberately omit many low-level details in this description, as they are well-known and can be found
in most introductions to positive supercompilation \cite{Sorensen1994TurchinSupercompiler,sorm98b,TMR/SCP2014}.
Using this definition of driving, plus the usual definitions of folding, whistle, and generalization, 
we can build configuration graphs of the form shown in Fig. \ref{fig:ConfGraph}.

\begin{figure}
\begin{lstlisting}
type ConfGraph =
  | CGLeaf of Exp
  | CGCon of ConName * list<ConfGraph>
  | CGUnfold of ConfGraph
  | CGCases of VarName * list<Pattern * ConfGraph>
  | CGFold of int * list<VarName * VarName>
  | CGLet of list<VarName * ConfGraph> * ConfGraph
\end{lstlisting}
\caption{Representation of a Configuration Graph}
\label{fig:ConfGraph}
\end{figure}

\subsection{Multi-result Driving And Generaliztion}

As already mentioned, a key difference in multi-result supercompilation is
that driving and generalization are grouped together: a \emph{multi-driving}
step can return not one, but several alternative configurations.
One of them is typically the result of standard driving, but the others
can be different kinds of generalizations.
The choice of generalization strategy depends on the intended use of the
multi-result supercompiler.
In our case, the main goal is to find a program of optimal size among the results.
Previous analyzes have shown that one of the main reasons for code size expolosion in
supercompilation is the unrestricted duplication of sub-expressions during driving.
Of course, sometimes such duplication pays off, as it leads to new opportunieties 
for optimization.
But this is not always the case.
These observations hint us to consider two guidning principles that should helps us attain this goal:
\begin{itemize}
  \item if standard driving can produce code duplication, provide also a generalized
    configuration, where no (non-trivial) sub-expressions are duplicated;
  \item if there is no risk of code duplication, avoid any generalization, as it will
    be unlikely to help with the size of the result.
\end{itemize}
To apply these principles, we analyze standard driving, case by case, to see where
we need to avoid code duplication by generalization.
In order to express generalization as a possible result of a driving step,
we extend our representation of a step result - Fig. \ref{fig:MultiDriveStepResult}.
The same figure shows the implementation of \verb|mdsrSubExps| that we have encountered earlier.
The source function \verb|multiDriveSteps| will be denoted $\mathtt{mrdrive}$ for brevity below.

\begin{figure}
\begin{lstlisting}
type MultiDriveStepResult =
  | MDSRLeaf of Exp
  | MDSRCon of ConName * list<Exp>
  | MDSRUnfold of Exp
  | MDSRCases of VarName * list<Pattern * Exp>
  | MDSRLet of list<VarName * Exp> * Exp

let mdsrSubExps (mdsr: MultiDriveStepResult) : list<Exp> =
  match mdsr with
  | MDSRLeaf _ -> []
  | MDSRCon(_, es) -> es
  | MDSRUnfold e -> [e]
  | MDSRCases(_, cases) -> List.map snd cases
  | MDSRLet(binds, e) -> e :: List.map snd binds
\end{lstlisting}
\caption{One Step of Multi-result Driving}
\label{fig:MultiDriveStepResult}
\end{figure}

\begin{itemize}
  \item The variable case is again trivial: $\mathtt{mrdrive} \llbracket x \rrbracket = [\mathtt{MDSRLeaf}(x)]$;
    (We use $[\ldots; \ldots; \ldots]$ to denote lists of results.)
  \item There is no code duplication when driving a constructor, so we again make no generalization: 
    $\mathtt{mrdrive} \llbracket C(e_1, \ldots, e_n) \rrbracket = [\mathtt{MDSRCon}(C, e_1, \ldots, e_n)]$;
  \item The unfolding of a call to an ordinary function can produce code duplication, if some arguments appear
    multiple times in the body of the definition. 
    We conservatively generalize all arguments of the call\footnote{We leave a more refined generalization treatment for future work}:
    $\mathtt{mrdrive} \llbracket f(e_1, \ldots, e_n) \rrbracket =$ 
      $[\mathtt{MDSRLet}(y_1=e_1, \ldots, y_n=e_n, e [ x_1\rightarrow y_1, \ldots, x_n\rightarrow y_n ]); $
      $\mathtt{MDSRUnfold}(e [ x_1\rightarrow e_1, \ldots, x_n\rightarrow e_n ])]$,
    where $y_1, \ldots, y_n$ are fresh and $f(x_1, \ldots, x_n) = e \in P$.
    Notice that we shall always place the generalization result before the driving result in the list.
    In this way, supercompiled programs produced earlier will have more generalizations;
  \item The case of a pattern-matching call with a known constructor is completely
    analogous to the previous one:
    $\mathtt{mrdrive} \llbracket g(C(e'_1, \ldots, e'_m), e_1, \ldots, e_n) \rrbracket = [$
    $\mathtt{MDSRLet}(u_1=e'_1, \ldots, u_m=e'_m, z_1=e_1, \ldots, z_n=e_n, e [ x_1\rightarrow u_1, \ldots, x_m\rightarrow u_m, y_1 \rightarrow z_1, \ldots, y_n \rightarrow z_n ]);$
    $\mathtt{MDSRUnfold}(e [ x_1\rightarrow e'_1, \ldots, x_m\rightarrow e'_m, y_1 \rightarrow e_1, \ldots, y_n \rightarrow e_n ])]$
    where $u_1, \ldots, u_m, z_1, \ldots, z_n$ are fresh and $g(C(x_1, \ldots, x_m), y_1, \ldots, y_n) \in P$;
  \item When we pattern-match on a variable, there can be some code duplication during
    information propagation. 
    The code potentially being duplicated, however, is always of the form $C(x_1, \ldots, x_n)$.
    We have currently decided to accept this limited form ot potential duplication, without adding a generalization:
    $\mathtt{mrdrive} \llbracket g(x, e_1, \ldots, e_n) \rrbracket =$
    $[\mathtt{MDSRCases}(x, \mathtt{propagate}(x, p_1, (e_1, \ldots, e_n), e'_1),$ $\ldots,$ \\
    $\mathtt{propagate}(x, p_m, (e_1, \ldots, e_n), e'_m))]$
    where $g(p_1, y_1, \ldots, y_n) = e'_1, \ldots, g(p_m, y_1, \ldots, y_n) = e'_m \in P$;
    
  \item The case of matching on a function call is perhaps the least obvious. 
    As during normal driving we reuse the result of driving the nested call, it is not statically clear what
    it will be.
    So we prefer to be conservative, and add a full generalization of the outer call here:
    $\mathtt{mrdrive} \llbracket g(f(e'_1, \ldots, e'_m), e_1, \ldots, e_n) \rrbracket =$
    $[\mathtt{MDSRLet}(x_0=f(e'_1, \ldots, e'_m), x_1=e_1, \ldots, x_n=e_n, g(x_0, \ldots, x_n));$ \\
    $\mathtt{mdsrMap}(\llbracket g(\bullet, e_1, \ldots, e_n) \rrbracket, \mathtt{mrdrive} \llbracket f(e'_1, \ldots, e'_m) \rrbracket)]$
    where $x_0, \ldots, x_n$ are, as usual, fresh.
\end{itemize}

To illustrate the process on a very simple example, consider the trivial program 
$P = \{ \mathtt{f(x,y)=Pair(x,y);} \}$. 
If we supercompile the expression \verb|f(A,B)|, we shall obtain the graph set shown in Fig. \ref{fig:TrivGraphSet},
which corresponds to 2 configuration graphs -- one with generalization, and one without.

\begin{figure}
\begin{lstlisting}
GSBuild
  ((MDSRUnfold f(A(), B()), f(A(), B())),
   [[GSBuild
      ((MDSRLet ([("x0", A()); ("y0", B())],Pair(x0, y0)), Pair(x0, y0)),
       [[GSBuild ((MDSRCon ("Pair",[x0; y0]), x0),[[]]);
         GSBuild ((MDSRCon ("Pair",[x0; y0]), y0),[[]])]]);
     GSBuild ((MDSRLet ([("x0", A()); ("y0", B())],Pair(x0, y0)), A()),[[]]);
     GSBuild ((MDSRLet ([("x0", A()); ("y0", B())],Pair(x0, y0)), B()),[[]])];
    [GSBuild
       ((MDSRUnfold Pair(A(), B()), Pair(A(), B())),
        [[GSBuild ((MDSRCon ("Pair",[A(); B()]), A()),[[]]);
          GSBuild ((MDSRCon ("Pair",[A(); B()]), B()),[[]])]])]])
\end{lstlisting}
\caption{The Graph Set of a Trivial Program}
\label{fig:TrivGraphSet}
\end{figure}

\section{Empirical Evaluation}\label{sec:EmpEval}

We have studied the behaviour of the proposed multi-result supercompiler on a few simple
examples, with a focus on the resulting program size.
A streightforward approach, which works for some of the smaller examples,
is to enumerate all resulting configuration graphs and gather statistics from them.
For one example -- the well-known ``KMP test'' -- this approach turned out to be too
time consuming.
So, to make possible to analyze larger examples, we adapted the approach 
of Grechanik et al. \cite{Romanenko2014StagedMRSC}, which permits to filter
the sequence of configuration graphs produced by MRSC, without explicitly
enumerating them.
In particular, we have implemented functions to extract:
\begin{itemize}
  \item the first configuration graph in the sequence (recall that by the ordering of 
    results during driving, it should contain the most generalizations);
  \item the last one -- with the least number of generalizations;
  \item the smallest graph;
  \item the largest graph.
\end{itemize}
The implementation of the last 2 functions is provided in Appendix \ref{app:FilterGraphSet},
the other 2 are similar.

\begin{figure*}
\begin{subfigure}[b]{\linewidth}
\begin{lstlisting}[language=Prolog]
append(Nil, ys) = ys;
append(Cons(x, xs), ys) = Cons(x, append(xs, ys));
\end{lstlisting}
\caption{List Append Program}
\label{sfig:AppPrg}
\end{subfigure}

\begin{subfigure}[b]{\linewidth}
\begin{lstlisting}[language=Lisp,keywords={}]
append(append(xs, ys), zs)
\end{lstlisting}
\caption{Double Append}
\label{sfig:DoubleApp}
\end{subfigure}

\begin{subfigure}[b]{\linewidth}
\begin{lstlisting}[language=Lisp,keywords={}]
not(True)  = False;
not(False) = True;

eqBool(True,  b) = b;
eqBool(False, b) = not(b);

match(Nil,         ss, op, os) = True;
match(Cons(p, pp), ss, op, os) = matchCons(ss, p, pp, op, os);

matchCons(Nil,         p, pp, op, os) = False;
matchCons(Cons(s, ss), p, pp, op, os) = matchHdEq(eqBool(p, s), pp, ss, op, os);

matchHdEq(True,  pp, ss, op, os) = match(pp, ss, op, os);
matchHdEq(False, pp, ss, op, os) = next(os, op);

next(Nil,         op) = False;
next(Cons(s, ss), op) = match(op, ss, op, ss);

isSublist(p, s) = match(p, s, p, s);
\end{lstlisting}
\caption{Substring Program}
\label{sfig:SubstrPrg}
\end{subfigure}

\begin{subfigure}[b]{\linewidth}
\begin{lstlisting}[language=Lisp,keywords={}]
isSublist(Cons(True, Cons(True, Cons(False, Nil))), s)
\end{lstlisting}
\caption{``KMP Test''}
\label{sfig:KMP}
\end{subfigure}

\begin{subfigure}[b]{\linewidth}
\begin{lstlisting}[language=Lisp,keywords={}]
eqBool(eqBool(x, y), eqBool(y, x))
\end{lstlisting}
\caption{Bool Equality Symmetry}
\label{sfig:BoolEqSym}
\end{subfigure}

\begin{subfigure}[b]{\linewidth}
\begin{lstlisting}[language=Lisp,keywords={}]
g(Nil,         y) = y;
g(Cons(x, xs), y) = f(g(xs, y));
f(w) = B(w, w);
\end{lstlisting}
\caption{Program Demonstrating Exponential Growth}
\label{sfig:ExpGrowthPrg}
\end{subfigure}

\begin{subfigure}[b]{\linewidth}
\begin{lstlisting}[language=Lisp,keywords={}]
g(Cons(A, Cons(A, Cons(A, Nil))), z)
\end{lstlisting}
\caption{Expression Demonstrating Exponential Growth}
\label{sfig:ExpGrowth}
\end{subfigure}

\caption{Example Programs}\label{fig:Examples}
\end{figure*}

The example programs and expressions are shown in Fig. \ref{fig:Examples}.
\begin{itemize}
  \item ``double append'' (Fig. \ref{sfig:AppPrg}-\subref{sfig:DoubleApp}) is traditionally used to 
    demonstrate the power of deforestation and supercompilation.
    In a way, it can also be seen as a proof that list append is associative.
  \item The ``KMP test'' (Fig. \ref{sfig:SubstrPrg}-\subref{sfig:KMP}) is another classical example, 
    which demonstrates the power of supercompilation with respect to deforestation and partial evaluation.
    It involves specializing a sub-list predicate to a fixed sub-list being
    searched in an unknown list.
  \item ``\verb|eqBool| symmetry'' (Fig. \ref{sfig:SubstrPrg}, \ref{sfig:BoolEqSym}) is intended
    to show that Boolean equality is symmetric.
  \item ``exp growth'' (Fig. \ref{sfig:ExpGrowthPrg}-\subref{sfig:ExpGrowth}) is an example taken 
    from S{\o}rensen's thesis \cite[Example 11.4.1]{Sorensen1994TurchinSupercompiler}, who attributes it to Sestoft.
    It is aimed to demonstrate how classical supercompilation can produce output programs
    growing exponentially with respect to the input.
\end{itemize}

The results of the multi-result supercompilation -- using our specific generalization approach --
are summarized in Table \ref{tbl:Stats}.
We give the configuration graph sizes for each of the four types of results (first, last, minimum/maximum size).

\begin{table}
  \centering
  \caption{MRSC Statistics}\label{tbl:Stats}
  \begin{tabular}{l|r|r|r|r}
    \hline
    Example                 & First & Last & Min. size & Max.size \\ \hline
    double append           &    12 &   10 &        10 &       19 \\
    KMP test                &   203 &   39 &        38 &     1055 \\
    \verb|eqBool| symmetry  &    16 &   17 &        16 &       30 \\
    exp growth              &    15 &   37 &        15 &       57 \\
  \end{tabular}
\end{table}

Several interesting observations stem from analyzing the selected resulting programs themselves:
\begin{itemize}
  \item In 2 cases (``\verb|eqBool| symmetry'' and ``exp growth'') the minimum program coincides with the first (most generalizing) one;
    in 1 case (``double append'') -- with the last (least generalizing) result. 
    In ``KMP test'' the difference between the minimum-size and the last program is minimal.
    This confirms the highly unpredictable impact of driving+generalization on program size.
  \item Note, however, that ``double append'' is a bit of an outlier -- it results in only 4 programs,
    one of which (the largest) is isomorphic to the input. The 2 smallest ones are the expected
    optimized version, shown in Fig. \ref{fig:DoubleAppResult}.
  \item In all cases, the size of the first result is closer to the minimum than to the maximum size.
    This confirms that our choice of generalization ensures limited growth of the result size.
  \item The smallest/last ``KMP test'' graph produces the expected optimal (as execution time) program,
    as shown in Fig. \ref{fig:KMPResult}.
  \item The last ``\verb|eqBool| symmetry'' graph produces a program, which can indeed serve as evidence of the
    symmetry of Boolean equality -- Fig. \ref{fig:BoolEqSymResult}.
  \item The results of ``exp growth'' are especially interesting in view of our main goal.
    The last result is the same as produced by S{\o}rensen's supercompiler --
    \verb|B(B(B(z, z), B(z, z)), B(B(z, z), B(z, z)))| -- clearly suffering form code-size explosion.
    The minimum-size (and also first) program -- shown in Fig. \ref{fig:ExpGrowthMinResult} --
    avoids the pitfall of code-size explosion, thanks to generalization.
    It, however, has also missed some opportunities for static evaluation.
    Interestingly, if we analyze the full set of results, there is another
    graph of size 17, which produces a program, which has eliminated all possible static reductions,
    while avoiding the risk of code explosion -- Fig. \ref{fig:ExpGrowthOptResult}.
    Apparently, we need a more refined approach for looking for (close to) minimum-size programs, which
    would not miss such results.
    One explanation of this discrepancy is that we compare configuration graph sizes, and not
    the sizes of the programs produced by residualizing these graphs.
    As residualization performs some optimizations (removing trivial lets, removing duplicated
    function definitions, etc.), the relation between graph size and resulting program size
    is not always a monotone function.
    On the other hand, it would be difficult to find the minimum-size \emph{program}
    without actually enumerating all such programs, which can be impractical in general.
    A possible compromize is to study better size measures for configuration graphs, instead
    of the simple node count we currently use.
    For example, ignoring unfolding nodes can give a better idea for the expected size
    of the residualized program, as unfolding nodes are ignored during residualization.
    
\end{itemize}

\begin{figure}
\begin{lstlisting}
f_0(ys, zs) = f_0_case0(ys, zs);
f_0_case0(Nil(), zs) = zs;
f_0_case0(Cons(x0, xs0), zs) = Cons(x0, f_0(xs0, zs));
f_(xs, ys, zs) = f__case0(xs, ys, zs);
f__case0(Nil(), ys, zs) = f_0(ys, zs);
f__case0(Cons(x00, xs00), ys, zs) = Cons(x00, f_(xs00, ys, zs));
expression: f_(xs, ys, zs)
\end{lstlisting}
\caption{Optimized double-append}
\label{fig:DoubleAppResult}
\end{figure}

\begin{figure}
\begin{lstlisting}
f_1_0_0_0_1_0_0(s0, ss1) = f_1_0_0_0_1_0_0_case0(s0, ss1);
f_1_0_0_0_1_0_0_case0(True(), ss1) = f_1_0_0_0_1_0_0_case1(ss1);
f_1_0_0_0_1_0_0_case0(False(), ss1) = f_0(ss1);
f_1_0_0_0_1_0_0_case1(Nil(), ) = False();
f_1_0_0_0_1_0_0_case1(Cons(s0, ss0), ) = f_1_0_0_0_1_0_0_case2(s0, s0, ss0);
f_1_0_0_0_1_0_0_case2(True(), s0, ss0) = f_1_0_0_0_1_0_0(s0, ss0);
f_1_0_0_0_1_0_0_case2(False(), s0, ss0) = True();
f_0(s) = f_0_case0(s);
f_0_case0(Nil(), ) = False();
f_0_case0(Cons(s0, ss0), ) = f_0_case1(s0, ss0);
f_0_case1(True(), ss0) = f_0_case2(ss0);
f_0_case1(False(), ss0) = f_0(ss0);
f_0_case2(Nil(), ) = False();
f_0_case2(Cons(s0, ss1), ) = f_1_0_0_0_1_0_0(s0, ss1);
expression: f_0(s))
\end{lstlisting}
\caption{Optimized KMP Test Result}
\label{fig:KMPResult}
\end{figure}

\begin{figure}
\begin{lstlisting}
main_case0(True(), y) = main_case2(y);
main_case0(False(), y) = main_case2(y);
main_case2(True(), ) = True();
main_case2(False(), ) = True();
expression: main_case0(x, y)
\end{lstlisting}
\caption{``\texttt{eqBool} symmetry'' Optimal Result}
\label{fig:BoolEqSymResult}
\end{figure}

\begin{figure}
\begin{lstlisting}
f_3(xs0, y0) = f_3_let0(f_3_case0(xs0, y0));
f_3_let0(w0) = B(w0, w0);
f_3_case0(Nil(), y0) = y0;
f_3_case0(Cons(x0, xs1), y0) = f_3(xs1, y0);
expression: f_3(Cons(A(), Cons(A(), Nil())), z))
\end{lstlisting}
\caption{``exp growth'' Minimum-size Result}
\label{fig:ExpGrowthMinResult}
\end{figure}

\begin{figure}
\begin{lstlisting}
main_let1(w0) = B(w0, w0);
expression: main_let1(main_let1(B(z, z)))
\end{lstlisting}
\caption{``exp growth'' Optimal Result}
\label{fig:ExpGrowthOptResult}
\end{figure}

\section{Future Work}

\section{Related Work}

\section{Conclusions}

%\nocite{*}
\bibliographystyle{eptcs}
\bibliography{MRScpOptSize}

\appendix

\clearpage
\section{Selecting Graphs from a $\mathtt{GraphSet}$}\label{app:FilterGraphSet}

\begin{lstlisting}[caption={Selecting a Graph of Minimum/Maximum Size from a Graph Set}]
let rec minMaxSizeGraph (cmp: int -> int -> bool) (g: GraphSet) : int * GraphSet =
  let selectMinMax (kx: int * 'A) (ky: int * 'A) : int * 'A =
    match kx, ky with
    | (-1, _), _ -> ky
    | _, (-1, _) -> kx
    | (k1, x), (k2, y) -> if cmp k1 k2 then kx else ky
  let minMaxSizeGraphs (gs: list<GraphSet>) : int * list<GraphSet> =
    (0, []) |> List.foldBack (fun g kgs -> 
      match minMaxSizeGraph cmp g, kgs with
      | (-1, g), (_, gs) -> (-1, g::gs)
      | (_, g), (-1, gs) -> (-1, g::gs)
      | (i, g), (j, gs) -> (i + j, g::gs)
      ) gs
  let minMaxSizeGraphss (gss: list<list<GraphSet>>) : int * list<GraphSet> =
    gss |> List.fold (fun kgs gs -> selectMinMax kgs (minMaxSizeGraphs gs)) (-1, [])
  match g with
  | GSNone -> (-1, GSNone)
  | GSFold _ -> (1, g)
  | GSBuild(c, gss) -> 
    match minMaxSizeGraphss gss with
    | -1, _ -> (-1, GSNone)
    | k, gs -> (1 + k, GSBuild(c, [gs]))
\end{lstlisting}


\end{document}
